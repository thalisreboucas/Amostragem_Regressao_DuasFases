---
title: "Estimador do tipo Regressão para amostragem Duas Fases"
author: "Thalis Rebouças"
subtitle: "CC0292 - 2024 | Setembro 2024 <br><br> Slides:**thalisreboucas.github.io/Amostragem_Regressao_DuasFases/#/**"
format: 
  revealjs:
    logo: "images/logo.png"
    width: 1600
    height: 900
    self-contained: false
    incremental: false
    footer: "Slides por Thalis Rebouças, feito em [Quarto](https://quarto.org/docs/presentations/revealjs/index.html). Código disponível [no GitHub](https://github.com/thalisreboucas/Amostragem_Regressao_DuasFases)."
    theme: ["custom.scss"]
    slide-number: c/t
    show-slide-number: all
    hash-type: number
    preview-links: false
---

## Sumário de aprendizagem

Vamos explicar um pouco sobre

-   Resumo Geral

     1. Fundamentos da amostragem em Duas Fases

     2. Fundamentos de Regressão linear

     3. Fundamentos do Estimador do tipo Regressão

-   Estimador do tipo Regressão para amostragem Duas Fases

     4. Explicação e aplicações

     5. Prova dos Estimadores

-   Exemplo prático

-   Referência

# Vamos lá!

# Fundamentos da amostragem em Duas Fases

## Fundamentos da <br> amostragem em Duas Fases:

<!--# Descrito por hugo -->

- Proposto por Neymann (1938) para seleção de amostras que não temos os conhecimentos pleno de como é composta estrutural a população
- Em Särndal & Sweensson (1987) que desenvolve a teoria da amostragem em duas fases de modo teórico e induzido pelos princípios do estimador de Horvitz-Thompson.
- Em Särndal, Swensson & Wretman (1992) propós duas opções de desenhos amostras eficiêntes para a seleção da amostra.

## Fundamentos da <br> amostragem em Duas Fases:

**Desenho proposto por Särndal, Swensson & Wretman (1992)** 

-   Usar um delineamento de amostragem simples e combiná-lo com o estimador de Horvitz-Thompson.

-   Obter informações sobre a população para construir uma nova estrutura amostral, podendo utilizar o estimador de regressão para aumentar a precisão.

Nesse caso, é feita a seleção de uma amostra na primeira fase, onde são selecionados os indivíduos e coletando as variáveis auxiliares, e depois é feita uma subamostra, selecionando novamente os indivíduos e coletando as variáveis de interesse.

<!--# Resumir Foi proposto por Neymann (1938) para seleção de amostras que não temos os conhecimentos pleno de como é composta estrutural a população.e isso se reflete em uma estrutura de amostragem inadequada que não contempla variáveis de informação auxiliar (de nenhum tipo: nem discreto, nem contínuo), e por isso, não é possível propor o uso de uma estratégia de amostragem ótima (delineamentos avançados proporcionais ao tamanho ou estratificados e estimadores de regressão ou de calibração) para a estimação dos parâmetros populacionais de interesse.  01-->

## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

- Na primeira fase, selecione uma amostra relativamente grande de elementos,$s_1$,colete informações de baixo custo sobre uma ou mais variáveis auxiliares , essas variáveis auxiliares serão utilizadas para estratificar a $s_1$.

- Com o auxílio das informações auxiliares coletadas na primeira fase, selecione uma amostra de segunda fase,$s_2$, a partir de $s_1$,utilizando o delineamento $p(.|s_1)$, com isso $s_2$ uma subamostra. A variável de estudo $y$ é então observada para os elementos na amostra da segunda fase.


## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

Consideramos uma população finita $U = \{1,...,k,...,N\}$, na qual temos $y$ senda a variável de interesse e $y_k$ será o y-ésimo elemento da população.

**Estimador do total** será,
$$t_y = \sum_Uy_k$$.

Para uma amostra $s$ o total será,$\sum_{k \in A}y_k$, se $A \subseteq U$


## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

Para a **primeira fase** teremos uma amostra $s1$ sendo $s1 \subset U$ de tamanho $n$, não fixa, retirada por meio de uma $p_a(s1)$, logo definimos as propabilidades de inclusão como:
$$\pi_{ak} = \sum_{k \in s}p_a(s1) ,\ \pi_{akl} = \sum_{k \in s}p_a(s1) , $$
Para $\pi_{akk}=\pi_{ak}$. Sendo $\Delta_{akl}=\pi_{akl}-\pi_{ak}\pi_{al}$, assumimos isso para $\pi_{ak}>0 , \forall k$ e para estimação da variânica $$\pi_{akl}>0 , \forall \  k \ne l$$


## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

Para a **segunda fase** teremos a amostra inicial $s1$ dada e teremos $s2 \subset s1$ com um tamanho $m$, não fixo, dado por $p(s2|s1)$,logo definimos as propabilidades de inclusão de $s2$ como:

$$\pi_{k|s1} = \sum_{k \in s2}p(s2|s1) ,\ \pi_{kl|s1} = \sum_{kl \in s}p_a(s2|s1) , $$
Para $\pi_{kk|s1}=\pi_{k|s1}$. Sendo $\Delta_{kl|s1}=\pi_{kl|s1}-\pi_{k|s1}\pi_{l|s1}$, assumimos isso para todo $s1$ $\pi_{k|s1}>0 , \forall \ k \in s1$ e para estimação da variânica $$\pi_{kl|s1}>0 , \forall \  k \ne l \in s1$$

## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

A soma expandida de $\pi^*$ definida, para todo $k,l \in s$ e para qualquer $s$,
$$\pi^*_k = \pi_{ak}\pi_{k|s} ,\ \pi^*_{kl} = \pi_{akl}\pi_{kl|s}$$

Com $\pi^*_{kk}=\pi^*_{k}$. Sendo $\Delta^*_{kl}=\pi^*_{kl}-\pi^*_{k}\pi^*_{l}$, definimos a expansão dos valores de $y$ e $\Delta$, sendo :
$$\check y_k = y_k/\pi_{ak} , \ \ \check y^+_k = \check y_k/\pi_{k|s} =y_k/\pi^*_k, \ \ \check \Delta_{akl} = \Delta_{akl}/\pi_{akl} $$ 
**obs:** "$\check{ }$" é a primeira fase de expansão e "$\check{ }$" e "$^+$" indica dupla expansão.

## Fundamentos da <br> amostragem em Duas Fases:

**Em Resumo:** 

**Os estimadores não viciados para,**

- $\hat{t}_{\pi^*} = \sum_r \check y^+_k = \sum_r y_k/\pi^*_k$

- $Var(\hat{t}_{\pi^*}) = \sum\sum_U\Delta_{akl} \check y_k \check y_l + E_a\{\sum\sum_s\Delta_{kl|s}\check y^+_k \check y^+_l\}$

- $\hat{Var}(\hat{t}_{\pi^*}) = \sum\sum_U\Delta_{akl} \check y_k \check y_l /\pi_{kl|S} + \sum\sum_s\Delta_{kl|s}\check y^+_k \check y^+_l /\pi_{kl|S}$


# Fundamentos de Regressão linear simples

## Fundamentos de <br> Regressão linear simples

- O MRLS tem a seguinte forma:
$$
 y_i = \beta_0 + \beta_1x_i + e_i , \forall_i = 1,\ldots,n.
$$
em que:
- $y_i$ é o valor da variável resposta e $x_i$ é o valor da variável explicativa, ambas referentes ao i-ésimo elemento da amostra.
- $\beta_0$ e $\beta_1$ são parâmentros desconhecidos, denominados parâmetros (coeficientes) de regressão.
- $e_i$ representa a fonte de variação associada ao i-ésimo elemento da amostra.

## Fundamentos de <br> Regressão linear simples

**Premissas**

1. A função de regressão é linear
2. $x_i$ são fixos e não são variáveis aleatórias. 
3. $E[e_i|x_i] = 0, \forall_i=1,\ldots,n.$ 
4. $Var[y_i|x_i] = Var[e_i|x_i] = \sigma^2, \forall_i=1,\ldots,n.$ (**Homoscesdaticidade**) 
5. $Cov(e_i,e_j) = E[e_ie_j] = 0, \ \forall i \neq j$
6. $e_j \sim  N(0,\sigma^2) ,i=1,\ldots,n.$ (**Para inferência de segunda ordem**)

## Fundamentos de <br> Regressão linear simples

**Estimador da Reta de regressão ajustada por MQ**

$$ \hat{y}_i = \hat{\beta}_0 +\hat{\beta}_1x_i , \ i,\ldots,n. $$

- $\hat{\beta_0} = \bar{y_n} - \hat{\beta_1}\cdot \bar{x_n}$

- $\hat{\beta_1} = \dfrac{\sum^n_{i=1} (x_i-\bar{x_n})(y_i-\bar{y_n})}{\sum^n_{i=1} (x_i-\bar{x_n})^2}$

- $\hat{e}_i = y_i-\hat{y}_i = yi - (\hat{\beta}_0 +\hat{\beta}_1x_i) , \ i,\ldots,n.$


# Fundamentos do Estimador do tipo Regressão

## Fundamentos do Estimador do tipo Regressão

Podemos pensar em um estimador que não tenta prever com exatidão o os valores dos dados,mas em uma estimador que estimar uma reta em uma nuvem de dados com uma correlação alta, em que a reta possa ou não passar pela origem.

Deste modo, podemos de maneira analoga comparar com o estimador `tipo razão` na qual funciona melhor se os dados forem bem ajustados por uma linha reta passando pela origem.

## Fundamentos do Estimador do tipo Regressão

**Estimador tipo regressão para uma amostra aleatória simples**

Com esse estimador esperamos, assim como o tipo razão, uma melhor precisão nas estimativas para $\bar{y}_U$, por conta da relação linear de $x$ e $y$.

Supondo que conheçamos a $\bar x_U$,a minha regressão estimada de $\bar{y}_U$ é ajustada quando $x=\bar x_U$:

$$
\hat y_{reg} = \hat \beta_0 + \hat \beta_1 \bar x_U= \bar y+\beta_1 (\bar x_U - \bar x)
$$
Se a média amostral $\bar{x}$ for menor que a média populacional $\bar{x}_U$ e $x$ e $y$ forem positivamente correlacionados, então esperaríamos que $\bar{y}$ também fosse menor que $\bar{y}_U$. 


## Fundamentos do Estimador do tipo Regressão


**Viés e erro quadrático médio para a estimativa por regressão.**

Assim como o estimador por razão, o estimador por regressão é `viesado`. 

Sejam $\beta_1$ e $\beta_0$ a inclinação e o intercepto da regressão por mínimos quadrados calculados a partir de todos os dados da população:

$$ \beta_1 = \frac{\sum_{i=1}^{N} (x_i − \bar{x}_U)(y_i − \bar{y}_U)}{\sum_{i=1}^{N} (x_i − \bar{x}_U)^2} = R \frac{S_y}{S_x}, $$
$$\beta_0 = \bar{y}_U − \beta_1 \bar{x}_U$$.

## Fundamentos do Estimador do tipo Regressão

**Viés e erro quadrático médio para a estimativa por regressão.**

O viés de $\hat{\bar{y}}_{reg}$ é dado por:

$$
E[\hat{\bar{y}}_{reg} − \bar{y}_U] = E[\bar{y} − \bar{y}_U] + E\left[B_1(\bar{x}_U − \bar{x})\right] = − \text{Cov}\left(B_1, \bar{x}\right)
$$

o viés é frequentemente desprezível em grandes amostras e se a linha de regressão passar por todos os pontos $(x_i, y_i)$ da população, então o viés é zero: nessa situação, $\hat{B}_1 = B_1$ para toda amostra, de modo que $\text{Cov}(\hat{B}_1, \bar{x}) = 0$. 

## Fundamentos do Estimador do tipo Regressão

**Viés e erro quadrático médio para a estimativa por regressão.**

Seja $e_i$ o i-ésimo resíduo do modelo de regressão ajustado para toda a população. Então,

$$MSE(\hat{y}_{reg}) = \left(1- \dfrac{n}{N} \right)\dfrac{S^2_d}{n} = \left(1- \dfrac{n}{N} \right)\dfrac{1}{n}S^2_d(1-R^2)$$
Assim, o Erro Quadrático Médio Aproximado (MSE) aproximado é pequeno quando:

## Fundamentos do Estimador do tipo Regressão

**Viés e erro quadrático médio para a estimativa por regressão.**

- $n$ é grande

- $\frac{n}{N}$ é grande

- $S_y$ é pequeno

- A correlação $R$ está próxima de -1 ou +1.

## Fundamentos do Estimador do tipo Regressão

**Em Resumo:** Um modelo base geral seria,

Modelo adotado: $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$

$$\hat{\beta}_1 = \frac{\sum_{i \in S} (x_i − \bar{x}_S) (Y_i − \bar{Y}_S)}{\sum_{i \in S} (x_i − \bar{x}_S)^2},$$

$$\hat{\beta}_0 = \bar{Y}_S − \hat{\beta}_1 \bar{x}_S,$$
$$\hat{T}_y  = N \left(\hat{\beta}_0 + \hat{\beta}_1 \bar{x}_U\right)$$

## Fundamentos do Estimador do tipo Regressão

**Em Resumo:** 
Na prática, se o tamanho da amostra for pequeno em relação ao tamanho da população e tivermos uma amostra aleatória simples (AAs), podemos simplesmente ignorar a correção para a população finita e usar o erro padrão para estimar a média da resposta.

$$
Var(Y_i) =
\sigma^2 \left[ \frac{1}{n} + \frac{(\bar{x}_U − \bar{x})^2}{\sum_{i \in S} (x_i − \bar{x})^2} \right].
$$
$$VM[\hat{T}_y − T] \approx N^2 \sigma^2 \left[ \frac{1}{n} + \frac{(\bar{x}_U − \bar{x}_S)^2}{\sum_{i \in S} (x_i − \bar{x}_S)^2} \right]., \ \text{se n/N for pequeno.}$$ 


# Estimador do tipo Regressão para amostragem Duas Fases

## Explicação

É um método de amostragem que utiliza informações auxiliares, coletadas juntamente com a amostra principal, para melhorar a precisão das estimativas dos valores totais e suas variâncias. Essas informações auxiliares podem incluir dados que estão relacionados com a variável de interesse, mas que são mais fáceis ou mais baratos de coletar.

Assim, o método não apenas se baseia na amostra observada, mas também aproveita as informações adicionais disponíveis para fazer inferências mais precisas sobre a população inteira.

## Explicação

 O Sandal, propõem um distinção em três situações dependendo da natureza da  informação auxiliar.
 
 1. O valor de $x_k=(x_{1k},...,x_{qk} )'$ do vetor auxiliar é $x=(x_1,...,x_q)'$ é registrado para as unidades $k \in s$.
 
 2. O valor de $x_k$ está disponível para todas as unidades $k$ em toda a população $U$.
 
 3. Uma combinação das Situações 1 e 2. O valor $xk$  é registrado para $k \in s$ e alguma outra informação (talvez uma relação 'mais fraca') $z_k=(z_{1k},...,z_{pk})'$ é conhecida para todas as unidades $k \in U$.

## Explicação

 **Desenvolvendo a situação 1 e 2**
De acordo com  (Cassel, Sarndal & Wretman, 1976; Särndal, 1982), temos que: 

- Supor a existência de uma relação de regressão entre $x_k$ e $y_k$ na população finita.

- $y_k$  será observado apenas em uma amostra de segunda fase relativamente pequena.(Por conta de escassez de informação).

Termos então um modelo linear $\xi$ da seguinte forma, com os $yk$ independentes:

- $E_\xi(y_k) = x_k\beta$
- $Var_\xi(y_k)=\sigma^2_k$ 


## Explicação

 **Desenvolvendo a situação 1 e 2**
 
Com isso, o estimador de mínimos quadrados ponderados de $\beta$ e dos resíduos será:

$$ \beta = (\sum_Ux_kx_k'/\sigma^2_k)^{-1}\sum_Ux_ky_k/\sigma^2_k) \ ,$$
$$\ E_k= y_k-x_k'\beta $$
Entretanto, $(y_k,x_k)$ são observadors para $k \in s2$ apenas e os k's carregam um peso de ${\pi^*_k}^{-1}$

## Explicação

 **Desenvolvendo a situação 1 e 2**

Portanto o $\hat{\beta}$ é estimado por:
$$
\hat{\beta} = \left(\sum_Ux_kx_k'/\sigma^2_k\pi^*_k)^{-1}\sum_Ux_ky_k/\sigma^2_k \pi^*_k \right).
$$
Deste modo, podemos utilizar esse estimador para calcular os valores preditos de $\hat{y} = x^{'}_k\hat{\beta}, \ \forall \ k \in s$, deste de que o $x_k$ seja conhecido para todo $k \in s$.Já os resíduos são dados por:

$$e_k = y_k - \hat y_k = y_k x^{'}_k\hat{\beta} \ \ (k \in s2)$$,
Deste de que $y_k$ seja conhecido para $k \in r$.

## Explicação

 **Desenvolvendo os estimadores - Situação 1**
 
Na amostragem de duas fases, quando $x_k$ é retirado para $k \in s$, o estimador aproximado não visiado para $t = \sum_U y_k$ é dado por:
$$\hat t_{1REG}=\sum_{s1} \hat y_k / \pi_{ak} + \sum_{s2}(y_k-\hat y_k)/\pi^*_k
$$
 A variância aproximada de $E_K$ e $\check E^+_k = E_k/\pi^*_k$ aproximada é dado por:
 $$AVar(\hat t_{1REG})=\sum\sum_U\Delta_{akl}\check y_k \check y_l + E_a \left\{ \sum\sum_{s1} \Delta_{kl|s1} \check E^+_k \check E^+_l \right\} $$

## Explicação

 **Desenvolvendo os estimadores - Situação 1**
 
A variância estimada aproxida de $e_K$ e $\check e^+_k = e_k/\pi^*_k$ aproximada é dado por:
 
 $$\hat{AVar}(\hat t_{1REG})=\sum\sum_{s2}\check \Delta_{akl}\check y_k \check y_l/\pi_{kl|s1}+  \sum\sum_{s2} \Delta_{kl|s1} \check e^+_k \check e^+_l /\pi_{kl|s1}$$

## Explicação

**Desenvolvendo os estimadores - Situação 2**
Na amostragem de duas fases, quando $x_k$ é retirado para $k \in U$, o estimador aproximado não visiado para $t = \sum_U y_k$ é dado por:


Na amostragem de duas fases, quando $x_k$ é retirado para $k \in s$, o estimador aproximado não visiado para $t = \sum_U y_k$ é dado por:
$$\hat t_{2REG}=\sum_{U} \hat y_k  + \sum_{s2}(y_k-\hat y_k)/\pi^*_k
$$
 A variância aproximada de $E_K$ e $\check E^+_k = E_k/\pi^*_k$ aproximada é dado por:
 $$AVar(\hat t_{2REG})=\sum\sum_U\Delta_{akl} \check E_k \check E_l + E_a  \sum\sum_{s1} \Delta_{kl|s1} \check E^+_k \check E^+_l  $$


## Explicação

 **Desenvolvendo os estimadores - Situação 2**
 
A variância estimada aproxida de $e_K$ e $\check e^+_k = e_k/\pi^*_k$ aproximada é dado por:
 
 $$\hat{AVar}(\hat t_{2REG})=\sum\sum_{s2}\check \Delta_{akl}\check e_k \check e_l/\pi_{kl|s1}+  \sum\sum_{s2} \Delta_{kl|s1} \check e^+_k \check e^+_l /\pi_{kl|s1}$$



## Explicação

**Desenvolvendo os estimadores - Situação 3**

Na amostragem de duas fases, quando $x_k$ é retirado para $k \in s$ e temos um $z_k$ para $k \in U$, o estimador aproximado não visiado para $t = \sum_U y_k$ é dado por:


Na amostragem de duas fases, quando $x_k$ é retirado para $k \in s$, o estimador aproximado não visiado para $t = \sum_U y_k$ é dado por:
$$\hat t_{3REG}=\sum_{U} \hat y_{1k}  + \sum_{s1}(y_k-\hat y_k)/\pi_{ak}+\sum_{s2}(y_k-\hat y_k)/\pi^*_k$$
A variância aproximada de $E_K$ e $\check E^+_k = E_k/\pi^*_k$ aproximada é dado por:
 $$AVar(\hat t_{2REG})=\sum\sum_U\Delta_{akl} \check E_{1k} \check E_{1l} + E_a  \sum\sum_{s1} \Delta_{kl|s1} \check E^+_k \check E^+_l  $$

## Explicação

**Desenvolvendo os estimadores - Situação 3**

A variância estimada aproxida de $e_K$ e $\check e^+_k = e_k/\pi^*_k$ aproximada é dado por:
 
 $$\hat{AVar}(\hat t_{3REG})=\sum\sum_{s2}\check \Delta_{akl}\check e_k \check e_l/\pi_{kl|s1}+  \sum\sum_{s2} \Delta_{kl|s1} \check e^+_k \check e^+_l /\pi_{kl|s1}$$


# Exemplo 1 - Särndal, Swensson (1987) 

## Exemplo 1 - Teórico

Utilizando uma amostragem em duas etapas para selecionar classes de estudantes são selecionados na primeira fase e os alunos individualmente são subamostras das classes selecionadas.Os estudantes selecionados formam a amostra da primeira fase , $s$, e váriavel $xk$ mais fraca como o aproveitamento acadêmico dos estudantes são coletadas.
Os pesos das amostras da fase um são $1/\pi_{ak} = 1/(\pi_{li}\pi_{ki})$,sendo $\pi_{li}$ a probabilidade de inclusão na primeira amostra e $\pi_{ki}$ é a probabilidade de selecionar novamente o k-ésimo individuo na segunda fase, sendo o i-ésimo individuo da primeira fase.Uma amostra da segunda fase, $r$, é extraída de $s$ por seleção aleatória simples de, por exemplo, $m$ dos $n$ k-ésimos individuos em $s$. Para $k \in r$, o valor $y_k$, uma medida mais robusta de desempenho, é registrado. 

## Exemplo 1 - Teórico

Em um caso em que o tamanho $N$ é a única informação disponível para a população como um todo, podemos aplicar a situação 3 com o modelo ‘inicial’.

$$E_{\xi1}(y_k) = \beta_1, \ V_{\xi1}(y_k) = \sigma^2_1 (k \in U)$$,

sendo $z_k = 1, \ \forall K$, os resultados estimados das regressões será:

$$
\hat t_{1REG}  = (\sum_s\check x_k)b, \ \ \hat t_{2REG}  = (\sum_U x_k)b, \ \  \hat t_{3REG}  = (N\bar x_k)b, 
$$
Sendo $\bar x_s = \sum_s \check x_k/\hat N$ , $\hat N= \sum_s1/\pi_{ak}$ , $b = (\sum_r\check y^+_k)/(\sum_r\check x^+_k)$  e $e_k = y_k - bx_k$.

# Estimador generalizado de regressão (greg) em duas fases
## Greg

# Prova dos Estimadores

# Exemplos 

## Exemplo do Livro Advanced Sampling Methods


**Exemplo 6.2 da página 81**

Selecione uma amostra de 20 unidades na primeira fase usando amostragem aleatória simples sem reposição e registre apenas a idade de uma pessoa em um bloco das unidades selecionadas na amostra. A partir da amostra da primeira fase de 20 unidades, selecione uma subamostra de 10 unidades e registre tanto a idade de uma pessoa quanto as horas de sono. Estime as horas médias de sono (anos) em um bloco usando um estimador de regressão em amostragem em duas fases. Deduzir o intervalo de confiança de 95%.

```{r}
# Idades
x <- c(45.84, 73.82, 64.49, 58.46, 73.62, 58.91, 75.70, 59.15, 78.20, 62.89,
       65.65, 64.94, 71.50, 62.92, 75.96, 74.63, 79.12, 65.80, 63.25, 70.06,
       67.10, 62.63, 72.67, 70.85, 84.86, 64.06, 70.33, 62.61, 76.42, 57.91,
       68.26, 58.82, 80.57, 77.63, 63.06, 71.14, 68.55, 84.53, 51.64, 76.62,
       64.89, 76.66, 63.50, 56.01, 70.84, 58.23, 72.68, 60.06, 68.19, 58.66)

# Número de horas de sono
y <- c(552.28, 367.02, 395.99, 477.71, 306.01, 435.09, 336.97, 359.99, 379.35, 403.77,
       366.45, 401.95, 403.39, 412.02, 309.47, 394.52, 352.69, 347.07, 388.45, 147.54,
       383.84, 400.17, 369.27, 361.34, 279.81, 421.00, 332.56, 397.37, 327.21, 425.10,
       436.99, 488.72, 280.05, 276.77, 381.98, 326.33, 383.53, 240.07, 434.50, 343.46,
       419.98, 310.65, 419.73, 454.06, 354.54, 395.14, 314.81, 425.12, 367.02, 412.70)

# Definindo a semente para reprodutibilidade
set.seed(3)

# Seleciona uma amostra de 20 unidades
s1 <- sample(1:50, 20, replace = FALSE)
X <- x[s1]
mX <- mean(X)

# Seleciona uma subamostra de 10 unidades da amostra anterior
S1 <- sample(s1, 10, replace = FALSE)
dx <- x[S1]
dy <- y[S1]

# Calcula as médias, variâncias e covariâncias
mdx <- mean(dx)
mdy <- mean(dy)
vdx <- var(dx)
vdy <- var(dy)
cdxdy <- cov(dx, dy)

# Estima o coeficiente beta da regressão
beta <- cdxdy / vdx

# Estimador de regressão para a média das horas de sono
ybrd <- mdy + beta * (mX - mdx)

# Calcula o erro quadrático médio
Mse <- ((1/20) - (1/50)) * vdy + ((1/10) - (1/20)) * vdy * (1 - cor(dx,dy)^2)

# Calcula o valor t para o intervalo de confiança de 95%
t <- qt(0.975, 9)

# Calcula os limites inferior e superior do intervalo de confiança
LL <- ybrd - t * sqrt(Mse)
UL <- ybrd + t * sqrt(Mse)

# Exibe os resultados
beta
ybrd
Mse
LL
UL


```
As horas médias de sono em um ano são 380,16 com um erro quadrático médio de 70,05, e um intervalo de confiança de 95% é [376,34, 414,21].


<!-- Estimating Lichen Biomass and Caribou Grazing on the Wintering Grounds of Northern Québec: An Application of Fire History and Landsat Data -->

## outra slide
O problema pede para realizar uma amostragem em duas fases para estimar a média de horas de sono por ano em um determinado bloco, utilizando um estimador de regressão.

Fase 1:
Amostragem Aleatória Simples Sem Reposição (SRSWOR): Selecione aleatoriamente 20 unidades (indivíduos) do bloco, sem permitir repetições. Anote apenas a idade de cada indivíduo selecionado nesta fase.

Utilização da Tabela 6.1: A tabela 6.1, mencionada no enunciado, provavelmente contém a lista completa de indivíduos no bloco e suas respectivas idades. Utilize esta tabela para selecionar aleatoriamente os 20 indivíduos da primeira fase.

Fase 2:
Subamostragem: Dos 20 indivíduos selecionados na Fase 1, selecione aleatoriamente 10 indivíduos, formando uma subamostra.
Coleta de Dados Completa: Para cada indivíduo na subamostra, anote tanto a idade quanto as horas de sono.

Estimativa e Intervalo de Confiança:
Estimador de Regressão: Utilize um estimador de regressão para estimar a média de horas de sono por ano para todo o bloco, usando as informações coletadas nas duas fases da amostragem. O estimador de regressão utiliza a relação entre idade e horas de sono observada na subamostra para melhorar a precisão da estimativa.

Intervalo de Confiança de 95%: Calcule um intervalo de confiança de 95% para a média estimada de horas de sono por ano. Este intervalo fornecerá uma faixa de valores plausíveis para a verdadeira média da população, com 95% de confiança.

Em resumo, o objetivo é estimar a média de horas de sono por ano em um bloco, utilizando um método de amostragem em duas fases com um estimador de regressão para aumentar a precisão da estimativa.


## outro slide 

$$\hat{y}_{reg}= \bar{y}+\beta(\bar{X}-\bar{x})$$
$$MSE=\left(\dfrac{1}{20}-\dfrac{1}{50}\right)v_y+\left(\dfrac{1}{10}-\dfrac{1}{20}\right)v_y(1-p^2)$$
$$IC = [\hat{y}_{reg}-t\sqrt{MSE},\hat{y}_{reg}+t\sqrt{MSE}]$$
Dados Utilizados:
Variável Auxiliar (x): Idade
Variável de Interesse (y): Número de horas de sono
Etapas de Análise:
Seleção da amostra inicial (20 unidades) por SRSWOR.
Seleção da subamostra (10 unidades) para coleta adicional de dados.
Cálculo dos estimadores de regressão.
Estimativa da média das horas de sono.
Cálculo do intervalo de confiança.



# Referências

- Särndal, Swensson & Wretman (1992). Model Assisted Survey Samplig.
- Raosaheb Latpate, Jayant Kshirsagar, Vinod Kumar Gupta, Girish Chandra (2021). Advanced Sampling Methods.
- Särndal, Swensson (1987). A General View of Estimation for Two Phases of Selection with Applications to Two-Phase Sampling and Nonresponse.
- Andrés Gutiérrez (2015). Estrategias de muestreo Diseño de encuestas y estimacion de parámetros.
- Shalabh, IIT Kanpur.Sampling Theory(https://home.iitk.ac.in/~shalab/course1.htm & https://nptel.ac.in/courses/111104073)
